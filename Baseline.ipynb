{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173ee626-2f50-4c71-b15d-c154fd6845dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T14:25:25.637006Z",
     "iopub.status.busy": "2024-09-28T14:25:25.636687Z",
     "iopub.status.idle": "2024-09-28T14:25:29.282388Z",
     "shell.execute_reply": "2024-09-28T14:25:29.281514Z",
     "shell.execute_reply.started": "2024-09-28T14:25:25.636983Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cc9b4c-65fc-4ad2-b886-44e8bc3ecf51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T14:25:33.977399Z",
     "iopub.status.busy": "2024-09-28T14:25:33.976917Z",
     "iopub.status.idle": "2024-09-28T14:25:33.981939Z",
     "shell.execute_reply": "2024-09-28T14:25:33.980676Z",
     "shell.execute_reply.started": "2024-09-28T14:25:33.977375Z"
    }
   },
   "outputs": [],
   "source": [
    "path_dataset = 'data/final_data/dataset_torch/'\n",
    "path_model_out = 'data/final_data/baseline/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba25dd9-9e05-47cd-86c6-ae0b29cad97c",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c99a2e6b-ac6d-4b98-ba24-146707b3bbaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T02:52:37.583749Z",
     "iopub.status.busy": "2024-09-28T02:52:37.582672Z",
     "iopub.status.idle": "2024-09-28T02:52:37.612721Z",
     "shell.execute_reply": "2024-09-28T02:52:37.611497Z",
     "shell.execute_reply.started": "2024-09-28T02:52:37.583694Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_metrics(X_test, y_test, model, model_name):\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy*100:.1f}\")\n",
    "    print(f\"Precision: {precision*100:.1f}\")\n",
    "    print(f\"Recall: {recall*100:.1f}\")\n",
    "    print(f\"F1 Score: {f1*100:.1f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # result = {}\n",
    "    # result['model'] = model_name\n",
    "    # result['accuracy'] = accuracy\n",
    "    # result['precision'] = precision\n",
    "    # result['recall'] =recall\n",
    "    # result['f1'] = f1\n",
    "    # return result\n",
    "    metrics_df = pd.DataFrame({\n",
    "            'Dataset': [model_name],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1 Score': [f1]\n",
    "        })\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "def plot_confusion_matrix(X_test, y_test, model):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from matplotlib.colors import Normalize\n",
    "\n",
    "    # 预测并输出混淆矩阵 \n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    row_sums = cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_percentage = cm / row_sums * 100\n",
    "    \n",
    "    # 创建图像和轴\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    # 控制cbar\n",
    "    norm = Normalize(vmin=0, vmax=100)\n",
    "    im = ax.imshow(cm_percentage, interpolation='nearest', cmap=plt.cm.Blues, norm=norm)\n",
    "    # 添加颜色条\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.ax.set_ylabel('Percentage', rotation=-90, va=\"bottom\")\n",
    "    # 设置轴的标签和其他属性\n",
    "    ax.set(\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        xticklabels=np.unique(y_test), \n",
    "        yticklabels=np.unique(y_test),\n",
    "        ylabel='True label',\n",
    "        xlabel='Predicted label'\n",
    "    )\n",
    "    # 旋转x轴标签，使其可读\n",
    "    plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # 在每个单元格中写入百分比值\n",
    "    fmt = '.1f'  # 保留一位小数点的格式\n",
    "    thresh = cm_percentage.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            text = '{:.1f}%'.format(cm_percentage[i, j])\n",
    "            ax.text(j, i, text,\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_percentage[i, j] > thresh else \"black\")\n",
    "            \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a9184-7c38-4719-8637-df20b1a0c8e1",
   "metadata": {},
   "source": [
    "## Data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d59569b-e752-41ce-a1e9-430e33938af7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:41:45.899367Z",
     "iopub.status.busy": "2024-09-27T16:41:45.898649Z",
     "iopub.status.idle": "2024-09-27T16:41:45.912965Z",
     "shell.execute_reply": "2024-09-27T16:41:45.911215Z",
     "shell.execute_reply.started": "2024-09-27T16:41:45.899312Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphToMLPDataset(Dataset):\n",
    "    def __init__(self, data, train_mask=None, val_mask=None):\n",
    "        self.x = data.x\n",
    "        self.y = data.y\n",
    "        self.train_mask = train_mask\n",
    "        self.val_mask = val_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train_mask is not None and self.val_mask is not None:\n",
    "            # 如果有mask，则根据mask返回训练或验证集的数据\n",
    "            if self.train_mask[idx]:\n",
    "                return self.x[idx], self.y[idx]\n",
    "            elif self.val_mask[idx]:\n",
    "                return self.x[idx], self.y[idx]\n",
    "        else:\n",
    "            # 如果没有mask，则返回所有数据\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "def get_dataset(in_dataset, model_name=None):\n",
    "    if model_name == 'node2vec':\n",
    "        out_datdaset = Data(\n",
    "            edge_index=in_dataset.edge_index,  \n",
    "            edge_attr=in_dataset.edge_attr\n",
    "        )\n",
    "        return out_datdaset\n",
    "    elif model_name == 'mlp':\n",
    "        out_datdaset = Data(\n",
    "            x = dataset.x,\n",
    "            y = dataset.y, \n",
    "            train_mask = dataset.train_mask,\n",
    "            val_mask = dataset.val_mask,\n",
    "            num_classes = dataset.num_classes\n",
    "        )\n",
    "        return out_datdaset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb9823-a38a-459a-bd9e-4b649ce3d87c",
   "metadata": {},
   "source": [
    "## Node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cf03f-c7bd-4627-803e-a6719cee08c3",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e234f3a-4ec2-4312-b0f2-ee66af25da04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:39:15.588932Z",
     "iopub.status.busy": "2024-09-27T16:39:15.588739Z",
     "iopub.status.idle": "2024-09-27T16:39:15.594867Z",
     "shell.execute_reply": "2024-09-27T16:39:15.594322Z",
     "shell.execute_reply.started": "2024-09-27T16:39:15.588912Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义Node2Vec以考虑边的权重\n",
    "class WeightedNode2Vec(Node2Vec):\n",
    "    def __init__(self, edge_index, edge_weight, *args, **kwargs):\n",
    "        super().__init__(edge_index, *args, **kwargs)\n",
    "        self.edge_weight = edge_weight\n",
    "\n",
    "    def random_walk(self, batch, walk_length):\n",
    "        row, col = self.adj_t.storage._row, self.adj_t.storage._col\n",
    "        rowptr, col, weight = self.adj_t.csr()\n",
    "        walk = torch.empty((batch.size(0), walk_length), dtype=torch.long, device=batch.device)\n",
    "        walk[:, 0] = batch\n",
    "\n",
    "        for i in range(1, walk_length):\n",
    "            neighbors = rowptr[walk[:, i-1].repeat_interleave(rowptr[walk[:, i-1]+1] - rowptr[walk[:, i-1]])] + col[rowptr[walk[:, i-1]]]\n",
    "            prob = self.edge_weight[rowptr[walk[:, i-1]].repeat_interleave(rowptr[walk[:, i-1]+1] - rowptr[walk[:, i-1]])]\n",
    "            prob = prob / prob.sum(dim=-1, keepdim=True)\n",
    "            walk[:, i] = neighbors[torch.multinomial(prob, 1).view(-1)]\n",
    "        \n",
    "        return walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52828498-a444-4686-8fa6-d31353f8dbf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:39:15.597914Z",
     "iopub.status.busy": "2024-09-27T16:39:15.597732Z",
     "iopub.status.idle": "2024-09-27T16:39:15.601777Z",
     "shell.execute_reply": "2024-09-27T16:39:15.601262Z",
     "shell.execute_reply.started": "2024-09-27T16:39:15.597895Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_node2vec(model, loader, epochs):\n",
    "    for epoch in range(epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451b0fa-5db3-4c0a-806d-c631d5276afa",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c2b92f-9eea-475f-b0b9-4ac7084c3a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T02:21:20.808817Z",
     "iopub.status.busy": "2024-09-28T02:21:20.808326Z",
     "iopub.status.idle": "2024-09-28T02:35:58.737723Z",
     "shell.execute_reply": "2024-09-28T02:35:58.736567Z",
     "shell.execute_reply.started": "2024-09-28T02:21:20.808783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai_2018.pth\n",
      "Train Node2vec...\n",
      "shanghai_2019.pth\n",
      "Train Node2vec...\n",
      "suzhou_2018.pth\n",
      "Train Node2vec...\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for i,dataset_ in enumerate([dataset for dataset in os.listdir(path_dataset) if dataset[-5:] != '1.pth' and dataset.endswith('.pth')]):\n",
    "    print(dataset_)\n",
    "    dataset = torch.load(path_dataset+dataset_)\n",
    "    dataset_node2vec = get_dataset(dataset, 'node2vec')\n",
    "\n",
    "    node2vec = WeightedNode2Vec(\n",
    "        dataset_node2vec.edge_index,\n",
    "        dataset_node2vec.edge_attr,\n",
    "        embedding_dim=128,\n",
    "        walks_per_node=10,\n",
    "        walk_length=80,\n",
    "        context_size=10,\n",
    "        p=1.0,\n",
    "        q=1.0,\n",
    "        num_negative_samples=1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(node2vec.parameters(), lr=0.01)\n",
    "    loader = node2vec.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    print('Train Node2vec...')\n",
    "    train_node2vec(node2vec, loader, epochs)\n",
    "    # torch.save(node2vec, path_model_out + dataset_.split('.')[0] + '_node2vec.pth')\n",
    "    \n",
    "    \n",
    "    embeddings = node2vec()\n",
    "    pd.DataFrame(embeddings.cpu().detach().numpy()).to_csv(path_model_out + dataset_.split('.')[0] + '_node2vec.csv', index=False)\n",
    "    \n",
    "    # train_embs = embeddings[dataset.train_mask].detach().cpu().numpy()\n",
    "    # train_y = dataset.y[dataset.train_mask].cpu().numpy()\n",
    "    \n",
    "    # val_embs = embeddings[dataset.val_mask].detach().cpu().numpy()\n",
    "    # val_y = dataset.y[dataset.val_mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26d2e4-60ae-496e-882f-cacd35eb3af4",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce1b09d7-79a9-4b93-aa63-cd04aa123196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T13:07:56.363753Z",
     "iopub.status.busy": "2024-09-27T13:07:56.363414Z",
     "iopub.status.idle": "2024-09-27T13:07:56.773969Z",
     "shell.execute_reply": "2024-09-27T13:07:56.773132Z",
     "shell.execute_reply.started": "2024-09-27T13:07:56.363730Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf', C=0.05, gamma='scale').fit(train_embs, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01dc2170-eda0-45dc-9b18-89f3ec7aca81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T02:47:37.734051Z",
     "iopub.status.busy": "2024-09-28T02:47:37.733628Z",
     "iopub.status.idle": "2024-09-28T02:47:37.739810Z",
     "shell.execute_reply": "2024-09-28T02:47:37.738635Z",
     "shell.execute_reply.started": "2024-09-28T02:47:37.734028Z"
    }
   },
   "outputs": [],
   "source": [
    "csvs = [\n",
    "    'data/final_data/baseline/shanghai_2018_node2vec.csv',\n",
    "    'data/final_data/baseline/shanghai_2019_node2vec.csv',\n",
    "    'data/final_data/baseline/suzhou_2018_node2vec.csv'\n",
    "]\n",
    "datasets = [\n",
    "    'data/final_data/dataset_torch/shanghai_2018.pth',\n",
    "    'data/final_data/dataset_torch/shanghai_2019.pth',\n",
    "    'data/final_data/dataset_torch/suzhou_2018.pth'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35222bd4-042e-4d5c-b102-62b90d1dee41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T02:57:51.016201Z",
     "iopub.status.busy": "2024-09-28T02:57:51.015500Z",
     "iopub.status.idle": "2024-09-28T02:57:51.255979Z",
     "shell.execute_reply": "2024-09-28T02:57:51.255083Z",
     "shell.execute_reply.started": "2024-09-28T02:57:51.016147Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = torch.load(datasets[2])\n",
    "df = pd.read_csv(csvs[2]).values\n",
    "\n",
    "train_embs = df[dataset.train_mask]\n",
    "train_y = dataset.y[dataset.train_mask].cpu().numpy()\n",
    "val_embs = df[dataset.val_mask]\n",
    "val_y = dataset.y[dataset.val_mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b64a3ecf-0437-47e4-ae12-59d600a67a2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T02:58:34.573120Z",
     "iopub.status.busy": "2024-09-28T02:58:34.572807Z",
     "iopub.status.idle": "2024-09-28T02:58:35.518757Z",
     "shell.execute_reply": "2024-09-28T02:58:35.518145Z",
     "shell.execute_reply.started": "2024-09-28T02:58:34.573098Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 15.5\n",
      "Precision: 14.7\n",
      "Recall: 17.0\n",
      "F1 Score: 13.2\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.43      0.28       368\n",
      "           1       0.16      0.10      0.12       368\n",
      "           2       0.07      0.23      0.10        88\n",
      "           3       0.14      0.15      0.14       190\n",
      "           4       0.37      0.12      0.18       773\n",
      "           5       0.10      0.09      0.10       193\n",
      "           6       0.10      0.04      0.06       379\n",
      "           7       0.11      0.05      0.07       274\n",
      "           8       0.08      0.31      0.12       130\n",
      "\n",
      "    accuracy                           0.15      2763\n",
      "   macro avg       0.15      0.17      0.13      2763\n",
      "weighted avg       0.20      0.15      0.15      2763\n",
      "\n",
      "Confusion Matrix:\n",
      " [[159  33  48  13  20  20  17  21  37]\n",
      " [132  38  43  21  27  18  16  33  40]\n",
      " [ 25   9  20   4   7   3   4   3  13]\n",
      " [ 41  14   9  29  16  19  16   3  43]\n",
      " [185  71  63  56  94  60  38  37 169]\n",
      " [ 40  12  18  25  20  18  17   4  39]\n",
      " [ 86  31  41  37  37  22  15  23  87]\n",
      " [ 82  25  47  16  20   9  14  15  46]\n",
      " [ 28  10  10   9  14   6  10   3  40]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='rbf', C=1, gamma='scale', decision_function_shape='ovo').fit(train_embs, train_y)\n",
    "df = output_metrics(val_embs, val_y, svc, csvs[2].split('/')[-1][:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0233a-f301-417c-89cb-9199d306fdce",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ec799-d485-4612-b52f-a1264f1aab47",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8efe0b6-98d2-41e7-9853-ab433e687421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:02:22.593371Z",
     "iopub.status.busy": "2024-09-27T16:02:22.592270Z",
     "iopub.status.idle": "2024-09-27T16:02:22.612048Z",
     "shell.execute_reply": "2024-09-27T16:02:22.610845Z",
     "shell.execute_reply.started": "2024-09-27T16:02:22.593316Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out): \n",
    "        super().__init__() \n",
    "        self.linear1 = nn.Linear(dim_in, dim_h) \n",
    "        self.linear2 = Linear(dim_h, dim_h) \n",
    "        self.linear3 = Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        x = F.relu(x) \n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "def train_mlp(model, data, criterion, optimizer, epochs, device):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _ = model(data.x)\n",
    "\n",
    "        out = F.log_softmax(_, dim=1)\n",
    "\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return losses\n",
    "\n",
    "def test_mlp(model, data, dataset_name):\n",
    "    _ = model(data.x)\n",
    "    out = F.log_softmax(_, dim=1)\n",
    "    \n",
    "    y_pred = out.argmax(dim=1)[data.val_mask].cpu()\n",
    "    y_true = data.y[data.val_mask].cpu()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "            'Dataset': [dataset_name],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1 Score': [f1]\n",
    "        })\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6bdef9-2d93-4913-b913-2acbffed734d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "109e2804-a9da-4e9b-bc5f-ff9cf336bfe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:02:33.321926Z",
     "iopub.status.busy": "2024-09-27T16:02:33.321249Z",
     "iopub.status.idle": "2024-09-27T16:03:09.014005Z",
     "shell.execute_reply": "2024-09-27T16:03:09.012897Z",
     "shell.execute_reply.started": "2024-09-27T16:02:33.321873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai_2018.pth\n",
      "Train MLP...\n",
      "shanghai_2019.pth\n",
      "Train MLP...\n",
      "suzhou_2018.pth\n",
      "Train MLP...\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.DataFrame()\n",
    "epochs = 2000\n",
    "\n",
    "for i,dataset_ in enumerate([dataset for dataset in os.listdir(path_dataset) if dataset[-5:] != '1.pth' and dataset.endswith('.pth')]):\n",
    "    print(dataset_)\n",
    "    dataset = torch.load(path_dataset+dataset_)\n",
    "    dataset_mlp = get_dataset(dataset, 'mlp')\n",
    "    \n",
    "    mlp = MLP(dataset_mlp.num_node_features, 256, dataset_mlp.num_classes).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "    print('Train MLP...')\n",
    "    loss = train_mlp(mlp, dataset_mlp, criterion, optimizer, epochs, device)\n",
    "    df = test_mlp(mlp, dataset_mlp, dataset_.split('.')[0] + '_mlp')\n",
    "    df_all = pd.concat([df_all, df])\n",
    "    torch.save(mlp, path_model_out + dataset_.split('.')[0] + '_mlp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dede38b5-3c0d-4e90-9f41-9b959441a23d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T15:13:57.180285Z",
     "iopub.status.busy": "2024-09-27T15:13:57.179767Z",
     "iopub.status.idle": "2024-09-27T15:13:57.190232Z",
     "shell.execute_reply": "2024-09-27T15:13:57.189275Z",
     "shell.execute_reply.started": "2024-09-27T15:13:57.180251Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all.to_csv('data/final_data/other/evaluation_mlp.csv' ,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0166b-513b-4333-8e6e-bcf05e338941",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e706ec-0fd1-4282-9a54-7432af38836f",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1fa9fa-c9a3-4627-888c-a92053ad933b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:14:57.080561Z",
     "iopub.status.busy": "2024-09-27T16:14:57.080153Z",
     "iopub.status.idle": "2024-09-27T16:14:57.089844Z",
     "shell.execute_reply": "2024-09-27T16:14:57.088713Z",
     "shell.execute_reply.started": "2024-09-27T16:14:57.080537Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out, num_layers=5):\n",
    "        super(GCN, self).__init__()\n",
    "        # 网络层数\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        # 输入层\n",
    "        self.convs.append(GCNConv(dim_in, dim_h))\n",
    "        \n",
    "        # 中间层\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(dim_h, dim_h))\n",
    "        \n",
    "        # 输出层\n",
    "        self.convs.append(GCNConv(dim_h, dim_out))\n",
    "\n",
    "        self.apply(self.weights_init)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                h = self.convs[i](x, edge_index, edge_weight)\n",
    "                h = F.relu(h)\n",
    "                h = F.dropout(h, p=0.6, training=self.training)\n",
    "                \n",
    "            elif i != (self.num_layers - 1):\n",
    "                h = self.convs[i](h, edge_index, edge_weight)\n",
    "                h = F.relu(h)\n",
    "                h = F.dropout(h, p=0.6, training=self.training)\n",
    "                \n",
    "            else:\n",
    "                h = self.convs[i](h, edge_index, edge_weight)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef85660-f13d-4fb1-b603-a5a5d99b8e69",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53b820e-ff4d-49e7-b979-4f6bbf05469c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T15:59:45.026327Z",
     "iopub.status.busy": "2024-09-27T15:59:45.025533Z",
     "iopub.status.idle": "2024-09-27T15:59:45.036214Z",
     "shell.execute_reply": "2024-09-27T15:59:45.034938Z",
     "shell.execute_reply.started": "2024-09-27T15:59:45.026304Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, save_model, epochs=200):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_accs = []\n",
    "    max_acc = 0\n",
    "\n",
    "    early_stopping_counter = 0\n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "        out = F.log_softmax(_, dim=1)\n",
    "\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = out.argmax(dim=1)[data.train_mask].cpu()\n",
    "        acc = accuracy_score(data.y[data.train_mask].cpu(), preds)\n",
    "\n",
    "        f1 = f1_score(data.y[data.train_mask].cpu(), preds, average='macro')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_ = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "            val_loss.append(val_loss_.item())\n",
    "            \n",
    "            _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "            out = F.log_softmax(_, dim=1)\n",
    "            \n",
    "            val_acc = accuracy_score(data.y[data.val_mask].cpu(), out.argmax(dim=1)[data.val_mask].cpu())\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "            if val_loss_ > min_val_loss:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= 10:\n",
    "                    print(\"早停机制\")\n",
    "                    break\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch:03d}/{epochs}], Train_Loss: {loss.item():0.3f}, Val_Loss: {val_loss[-1]:0.3f}, Val_acc: {max(val_accs):.3f}')\n",
    "            val_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76d7aa4-6779-469c-a964-34c0a08a8360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:20:25.268363Z",
     "iopub.status.busy": "2024-09-27T16:20:25.267671Z",
     "iopub.status.idle": "2024-09-27T16:20:25.279706Z",
     "shell.execute_reply": "2024-09-27T16:20:25.278496Z",
     "shell.execute_reply.started": "2024-09-27T16:20:25.268310Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_gcn(model, data, dataset_name):\n",
    "    model.eval()\n",
    "    _ = model(data.x, data.edge_index, data.edge_attr)\n",
    "    out = F.log_softmax(_, dim=1)\n",
    "    \n",
    "    y_pred = out.argmax(dim=1)[data.val_mask].cpu()\n",
    "    y_true = data.y[data.val_mask].cpu()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "            'Dataset': [dataset_name],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1 Score': [f1]\n",
    "        })\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d422324a-dc18-4222-866e-0232317d7f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T15:59:47.772368Z",
     "iopub.status.busy": "2024-09-27T15:59:47.771742Z",
     "iopub.status.idle": "2024-09-27T16:00:41.015844Z",
     "shell.execute_reply": "2024-09-27T16:00:41.015072Z",
     "shell.execute_reply.started": "2024-09-27T15:59:47.772317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suzhou_2018.pth\n",
      "Epoch [000/250], Train_Loss: 2.197, Val_Loss: 2.196, Val_acc: 0.256\n",
      "Epoch [010/250], Train_Loss: 2.053, Val_Loss: 2.090, Val_acc: 0.339\n",
      "Epoch [020/250], Train_Loss: 1.734, Val_Loss: 1.817, Val_acc: 0.450\n",
      "Epoch [030/250], Train_Loss: 1.471, Val_Loss: 1.559, Val_acc: 0.529\n",
      "Epoch [040/250], Train_Loss: 1.331, Val_Loss: 1.408, Val_acc: 0.574\n",
      "Epoch [050/250], Train_Loss: 1.237, Val_Loss: 1.310, Val_acc: 0.607\n",
      "Epoch [060/250], Train_Loss: 1.163, Val_Loss: 1.274, Val_acc: 0.638\n",
      "Epoch [070/250], Train_Loss: 1.110, Val_Loss: 1.223, Val_acc: 0.650\n",
      "Epoch [080/250], Train_Loss: 1.070, Val_Loss: 1.187, Val_acc: 0.674\n",
      "Epoch [090/250], Train_Loss: 1.034, Val_Loss: 1.182, Val_acc: 0.676\n"
     ]
    }
   ],
   "source": [
    "layers = [3,3,3]\n",
    "dataset_name = [dataset for dataset in os.listdir(path_dataset) if dataset[-8:] != 'bert.pth' and dataset.endswith('.pth')]\n",
    "\n",
    "i = 0\n",
    "dataset_ = dataset_name[i]\n",
    " \n",
    "print(dataset_)\n",
    "dataset_gcn = torch.load(path_dataset+dataset_)\n",
    "gcn = GCN(dataset_gcn.num_node_features, dataset_gcn.num_node_features, dataset_gcn.num_classes, layers[i]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "train(gcn, dataset, optimizer, criterion, path_model_out+dataset_, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6c9fe-11c4-4aee-b9c4-2bf946e0044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33015ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_gcn(gcn, dataset_gcn, dataset_.split('.')[0] + '_mlp')\n",
    "df_all = pd.concat([df_all,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d298fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('data/final_data/other/evaluation_gcn.csv' ,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ufz39",
   "language": "python",
   "name": "torch39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
